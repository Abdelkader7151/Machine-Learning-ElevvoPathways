{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Student Performance Indicator\n",
        "\n",
        "This notebook builds a simple regression model to predict students' exam scores and explores key factors.\n",
        "\n",
        "- Load dataset from `data/` (CSV/XLS/XLSX automatically detected)\n",
        "- Quick EDA and cleaning\n",
        "- Visualizations: distribution, study hours vs score, and a well-organized correlation matrix\n",
        "- Linear Regression model with evaluation metrics (RÂ², RMSE, MAE)\n",
        "- Feature importance overview\n",
        "- Optional: Polynomial regression on study hours\n",
        "\n",
        "> Dataset used: any file in `data/` that matches common student-performance names (e.g., `Student_Performance_Factors.csv`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and utility functions\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Optional, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "\n",
        "def find_dataset_path() -> Optional[Tuple[str, str]]:\n",
        "\tdata_dir = 'data'\n",
        "\tpreferred = os.path.join(data_dir, 'Student_Performance_Factors.csv')\n",
        "\tif os.path.exists(preferred):\n",
        "\t\treturn preferred, 'csv'\n",
        "\tif not os.path.isdir(data_dir):\n",
        "\t\treturn None\n",
        "\tcandidates = []\n",
        "\tfor root, _, files in os.walk(data_dir):\n",
        "\t\tfor name in files:\n",
        "\t\t\tlower = name.lower()\n",
        "\t\t\tif lower.endswith('.csv'):\n",
        "\t\t\t\tcandidates.append((os.path.join(root, name), 'csv', lower))\n",
        "\t\t\telif lower.endswith('.xlsx') or lower.endswith('.xls'):\n",
        "\t\t\t\tcandidates.append((os.path.join(root, name), 'excel', lower))\n",
        "\tif not candidates:\n",
        "\t\treturn None\n",
        "\tkeywords = ['student', 'performance', 'score', 'exam']\n",
        "\tdef score(n: str) -> int:\n",
        "\t\treturn sum(1 for k in keywords if k in n)\n",
        "\tbest = max(candidates, key=lambda t: (score(t[2]), t[2]))\n",
        "\treturn best[0], best[1]\n",
        "\n",
        "\n",
        "def infer_target_column(df: pd.DataFrame) -> Optional[str]:\n",
        "\tname_priority = ['exam_score', 'score', 'marks', 'mark', 'grade', 'final', 'result']\n",
        "\tlower_map = {c.lower(): c for c in df.columns}\n",
        "\tfor key in name_priority:\n",
        "\t\tfor col_lower, original in lower_map.items():\n",
        "\t\t\tif key in col_lower and pd.api.types.is_numeric_dtype(df[original]):\n",
        "\t\t\t\treturn original\n",
        "\tif 'Exam_Score' in df.columns:\n",
        "\t\treturn 'Exam_Score'\n",
        "\tnumeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\treturn numeric_cols[0] if numeric_cols else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "found = find_dataset_path()\n",
        "if not found:\n",
        "\traise FileNotFoundError(\"Dataset not found. Place CSV/XLSX in data/.\")\n",
        "path, fmt = found\n",
        "if fmt == 'csv':\n",
        "\tdf = pd.read_csv(path)\n",
        "else:\n",
        "\tdf = pd.read_excel(path)\n",
        "print(f\"Loaded: {path} | shape={df.shape}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic cleaning\n",
        "# Attempt numeric conversion for object columns\n",
        "for c in df.columns:\n",
        "\tif df[c].dtype == 'object':\n",
        "\t\tconv = pd.to_numeric(df[c], errors='ignore')\n",
        "\t\tif not conv.equals(df[c]):\n",
        "\t\t\tdf[c] = conv\n",
        "\n",
        "# Impute missing values\n",
        "for c in df.columns:\n",
        "\tif pd.api.types.is_numeric_dtype(df[c]):\n",
        "\t\tdf[c] = df[c].fillna(df[c].median())\n",
        "\telse:\n",
        "\t\tif df[c].isnull().any():\n",
        "\t\t\tm = df[c].mode(dropna=True)\n",
        "\t\t\tif not m.empty:\n",
        "\t\t\t\tdf[c] = df[c].fillna(m.iloc[0])\n",
        "\n",
        "df.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target selection\n",
        "if 'Exam_Score' in df.columns:\n",
        "\ttarget = 'Exam_Score'\n",
        "else:\n",
        "\ttarget = infer_target_column(df)\n",
        "\tassert target is not None, \"No numeric target found\"\n",
        "print('Target:', target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 4)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(df[target], bins=20, alpha=0.7)\n",
        "plt.title(f'Distribution of {target}')\n",
        "plt.xlabel(target)\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "study_col = None\n",
        "for col in df.columns:\n",
        "\tif 'hour' in col.lower() and 'stud' in col.lower():\n",
        "\t\tstudy_col = col\n",
        "\t\tbreak\n",
        "\n",
        "if study_col:\n",
        "\tplt.subplot(1, 3, 2)\n",
        "\tplt.scatter(df[study_col], df[target], alpha=0.6)\n",
        "\tplt.title(f'{study_col} vs {target}')\n",
        "\tplt.xlabel(study_col)\n",
        "\tplt.ylabel(target)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "numeric_df = df.select_dtypes(include=[np.number])\n",
        "if numeric_df.shape[1] >= 2:\n",
        "\tcorr = numeric_df.corr(numeric_only=True)\n",
        "\tif target in corr.columns:\n",
        "\t\torder = corr[target].abs().sort_values(ascending=False).index.tolist()\n",
        "\t\ttop_n = min(8, len(order))\n",
        "\t\torder = order[:top_n]\n",
        "\t\tif target in order:\n",
        "\t\t\torder = [c for c in order if c != target] + [target]\n",
        "\t\tcorr = corr.loc[order, order]\n",
        "\tsns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1, center=0, square=True,\n",
        "\t\t\tlinewidths=0.5, linecolor='lightgray', cbar_kws={'shrink': 0.7})\n",
        "\tplt.xticks(rotation=45, ha='right')\n",
        "\tplt.yticks(rotation=0)\n",
        "\tplt.title('Correlation Matrix')\n",
        "else:\n",
        "\tplt.text(0.5, 0.5, 'Not enough numeric columns', ha='center', va='center')\n",
        "\tplt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modeling\n",
        "numeric_features = df.select_dtypes(include=[np.number])\n",
        "if target in numeric_features.columns:\n",
        "\tX = numeric_features.drop(columns=[target])\n",
        "else:\n",
        "\tdf[target] = pd.to_numeric(df[target], errors='coerce').fillna(df[target].median())\n",
        "\tnumeric_features = df.select_dtypes(include=[np.number])\n",
        "\tX = numeric_features.drop(columns=[target], errors='ignore')\n",
        "y = df[target]\n",
        "\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "if len(cat_cols) > 0:\n",
        "\tdf_enc = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
        "\tX = df_enc.select_dtypes(include=[np.number]).drop(columns=[target], errors='ignore')\n",
        "\ty = df_enc[target]\n",
        "\n",
        "assert X.shape[1] > 0, 'No usable features after preprocessing'\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print({'R2': r2, 'RMSE': rmse, 'MAE': mae})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predictions plots\n",
        "fig = plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test, y_pred, alpha=0.6)\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Actual vs Predicted')\n",
        "min_val = min(y_test.min(), y_pred.min())\n",
        "max_val = max(y_test.max(), y_pred.max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=1)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals = y_test - y_pred\n",
        "plt.hist(residuals, bins=20, alpha=0.7)\n",
        "plt.title('Residuals Distribution')\n",
        "plt.xlabel('Residual')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
